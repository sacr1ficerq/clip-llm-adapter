## Что это за репозиторий?

В этом проекте я собрал пайплайн для image captioning на датасете `Flickr8k`, объединив замороженный CLIP-энкодер и замороженную Qwen3-0.6B через собственный адаптер, обучая только этот адаптер.  

Я реализовал `VisionAdapter` для преобразования выходов CLIP в фиксированное число визуальных токенов в пространстве Qwen.
Структура адаптера:

``` python
    x[:, 1:, :].transpose(1, 2) # drop CLS token
    → Conv1d(kernel_size=3, stride=2, padding=1)
    → AdaptiveAvgPool1d(n_tokens)
    → transpose(1, 2)
    → LayerNorm(vision_dim)
    → Linear(vision_dim → qwen_dim)
    → LayerNorm(qwen_dim)
```

Я написал обертку над датасетом и другие вспомогательный функции для обучения.
Также я реализовал небольшой класс `QwenVisionCaptionTrainer`, который строит входы для Qwen из визуальных и текстовых эмбеддингов, считает лосс только по тексту и поддерживает обучение, валидацию и генерацию капшенов.
