{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzWpMk0BTfno"
   },
   "source": [
    "# ДЗ2. Мультимодальный адаптер к Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOtDtoFXTmY5"
   },
   "source": [
    "**Описание задания**\n",
    "\n",
    "В этом задании вы подключите внешнюю модальность (аудио или изображение) к языковой модели Qwen3-0.6B через небольшой обучаемый адаптер. Веса Qwen и выбранного предобученного энкодера мы замораживаем, обучается только адаптер.\n",
    "\n",
    "- Трек B (изображение → текст)\n",
    "\n",
    "Описываем изображения (image captioning датасет) с помощью Qwen. Энкодер: любая vision-модель.\n",
    "\n",
    "**Задачи:**\n",
    "\n",
    "1.   Заморозить параметры `QWEN` и предобученного энкодера (аудио или vision).\n",
    "\n",
    "2.   Создать и обучить адаптер, который сжимает временную / пространственную размерность признаков и проецирует их в скрытое пространство `Qwen`.\n",
    "\n",
    "3.   Подготовить данные для обучения и валидации модели.\n",
    "\n",
    "4.   Реализовать и сравнить несколько стратегий пулинга в адаптере (например, сжатие временной размерности для аудио или пространственной - для изображений).\n",
    "\n",
    "5.   Использовать `BERTScore` для оценки качества модели на отложенном датасете.\n",
    "\n",
    "**Основные этапы задания**\n",
    "\n",
    "*   Подготовка данных: для \"изображение → текст\" загрузите датасет для image captioning (например, `Flickr8k`), обработайте данные и создайте DataLoader для батчевого обучения. Рекомендуется выполнить полную предобработку данных (прекомпьют), чтобы сократить время обработки на этапе обучения.\n",
    "*   Реализация адаптера: создайте класс, который сжимает последовательность векторов.\n",
    "*   Интеграция с `QWEN`: реализуйте обработку входов и передачу через `QWEN`.\n",
    "*   Обучение модели: настройте процесс обучения с использованием `Cross Entropy Loss` и teacher forcing.\n",
    "*   Оценка: вычислите BERTScore между сгенерированными и реальными текстами на валидационном датасете.\n",
    "\n",
    "> **Внимание!** Последующие ячейки с условиями оформлены (название классов и переменных, инструкциии и комментарии) в ключе работы по треку А (аудио → текст). Если вы выбираете работать с vision задачей, можете ориентироваться содержательно на представленные кодовые сниппеты, но видоизменять их под свою задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ek97gBqjS9U"
   },
   "source": [
    "### Сеттинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JbyKDHKueNBa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloexec/Desktop/HSE/multimodal/HW1/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bert_score import BERTScorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMNWATzFYh67",
    "outputId": "cc5b8b5d-c26f-430d-9784-ab41c92ccf61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ8xbIDFkYy6"
   },
   "source": [
    "**Загрузка данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMH2kpEDbCmU"
   },
   "source": [
    "Если вы выбираете трек с аудио, используйте датасет `AudioCaps`.\n",
    "\n",
    "Я буду использовать Flickr8k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H-IwBsjZ8oBX"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# use flickr8k dataset\n",
    "\n",
    "ds = load_dataset(\"jxie/flickr8k\").with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[ 38,  64,  78,  ...,  52,  60,  41],\n",
       "          [ 59,  91,  54,  ...,  37,  57,  60],\n",
       "          [ 90,  79,  82,  ...,  43,  36,  18],\n",
       "          ...,\n",
       "          [223, 218, 211,  ..., 215, 215, 210],\n",
       "          [241, 234, 233,  ..., 231, 218, 205],\n",
       "          [237, 244, 241,  ..., 226, 226, 223]],\n",
       " \n",
       "         [[ 31,  50,  73,  ...,  48,  56,  37],\n",
       "          [ 37,  59,  34,  ...,  36,  52,  50],\n",
       "          [ 66,  50,  67,  ...,  28,  29,  18],\n",
       "          ...,\n",
       "          [223, 220, 213,  ..., 217, 217, 212],\n",
       "          [243, 236, 238,  ..., 241, 228, 214],\n",
       "          [239, 248, 247,  ..., 230, 229, 226]],\n",
       " \n",
       "         [[ 25,  49,  67,  ...,  37,  47,  28],\n",
       "          [ 49,  60,  25,  ...,  32,  49,  49],\n",
       "          [ 56,  46,  60,  ...,  21,  21,   8],\n",
       "          ...,\n",
       "          [221, 217, 210,  ..., 229, 229, 225],\n",
       "          [240, 233, 234,  ..., 242, 230, 221],\n",
       "          [238, 247, 245,  ..., 239, 238, 233]]], dtype=torch.uint8),\n",
       " 'caption_0': 'A black dog is running after a white dog in the snow .',\n",
       " 'caption_1': 'Black dog chasing brown dog through snow',\n",
       " 'caption_2': 'Two dogs chase each other across the snowy ground .',\n",
       " 'caption_3': 'Two dogs play together in the snow .',\n",
       " 'caption_4': 'Two dogs running through a low lying body of water .'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YGacOadeVci"
   },
   "source": [
    "### Задание 1. Создание и обучение AudioConvAdapter (3 балла)\n",
    "\n",
    "1. Загрузите модель **Qwen3-0.6B** и заморозьте её параметры.  \n",
    "2. Загрузите предобученный аудио-энкодер (параметры также должны быть заморожены). Вы можете выбрать одну из следующих моделей: **HuBERT**, **wav2vec2**, или **Whisper Encoder**. Учтите особенности выбранной модели:  \n",
    "   - Например, `wav2vec2-large-960h-lv60-self` использует `flash_attention`, которая работает только с GPU архитектуры Ampere и с типом данных float16. Это может вызвать сложности, если используется другое оборудование.  \n",
    "3. Реализуйте класс `AudioConvAdapter`, который уменьшает размерность последовательности аудио по времени и переводит её в текстовое пространство модели QWEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4tYDMIDwYkAz"
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "qwen_model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "qwen_tokenizer =  AutoTokenizer.from_pretrained(qwen_model_name, use_fast=True)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(qwen_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello world! This is a simple example of the web application. It's a good idea to have a simple web\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "txt = \"Hello world!\"\n",
    "tok = qwen_tokenizer(txt, return_tensors='pt')\n",
    "tok['input_ids'] = tok['input_ids'].to(device)\n",
    "tok['attention_mask'] = tok['attention_mask'].to(device)\n",
    "out = qwen_model.generate(**tok)\n",
    "qwen_tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze\n",
    "qwen_model.eval()\n",
    "for p in qwen_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4hEbt8GXFhaP"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, CLIPVisionModel\n",
    "\n",
    "# Load\n",
    "vision_model_name = \"openai/clip-vit-base-patch32\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_model_name, use_fast=True)\n",
    "vision_encoder = CLIPVisionModel.from_pretrained(vision_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze\n",
    "vision_encoder.eval()\n",
    "for p in vision_encoder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ckqo6GCxcmfD"
   },
   "outputs": [],
   "source": [
    "#TODO: имплементируйте конструктор и метод forward. Добавьте необходимые аргументы в конструктор\n",
    "# class AudioConvAdapter(nn.Module):\n",
    "\n",
    "# Пример максимально упрощённого адаптера из 4 блоков:\n",
    "# relu(Conv1D(in, in)) -> Linear(in, hid) -> relu(Conv1D(hid, hid)) -> Linear(hid, qwen_in)\n",
    "# где in - размер аудио вектора\n",
    "# hid - скрытое состояние адаптера (hid > in)\n",
    "# qwen_in - размерность хиддена qwen\n",
    "#   - первый Conv1D уменьшает число временных шагов (stride/pooling),\n",
    "#   - затем Linear преобразует hidden_dim,\n",
    "#   - потом снова Conv1D (доп. pooling),\n",
    "#   - потом Linear подгоняет к нужной размерности.\n",
    "# Можно без линейных слоев увеличивать размерность, но получится больше параметров\n",
    "# Можете реализивать любую свою архитектуру.\n",
    "\n",
    "# I will use vision though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.adapter import VisionAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gSzOFB4Wbtao"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_hidden = qwen_model.config.hidden_size\n",
    "vision_hidden = vision_encoder.config.hidden_size\n",
    "\n",
    "vision_hidden, qwen_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PoXrO9YFcs2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in adapter: 2,561,280\n"
     ]
    }
   ],
   "source": [
    "adapter_hidden_dim = 1024  # внутренняя размерность adapter’а\n",
    "vision_adapter = VisionAdapter(vision_hidden, qwen_hidden)# your code here\n",
    "\n",
    "print(\"Trainable parameters in adapter: {:,}\".format(sum(p.numel() for p in vision_adapter.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoFhT78mpSGZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Задание 2. Подготовка данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGP_y2mFBxxl"
   },
   "outputs": [],
   "source": [
    "def fix_tsv_file(tsv_in, tsv_out):\n",
    "    with open(tsv_in, \"r\", encoding=\"utf-8\") as fin, open(tsv_out, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) > 4:\n",
    "                fixed_line = \"\\t\".join(parts[:3]) + \"\\t\" + \" \".join(parts[3:]).strip()\n",
    "                fout.write(fixed_line + \"\\n\")\n",
    "            else:\n",
    "                fout.write(line)\n",
    "\n",
    "fix_tsv_file(\"/content/audiocaps/audiocaps/audiocaps_train.tsv\",\n",
    "             \"/content/audiocaps/audiocaps/audiocaps_train_fixed.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnlU_CsvWwV7"
   },
   "outputs": [],
   "source": [
    "class AudioCapsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tsv_path: str,\n",
    "        root_dir: str,\n",
    "        max_audio_length: int = 16000 * 10,  # 10 секунд при 16kHz\n",
    "        target_sample_rate: int = 16000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        tsv_path: путь к .tsv (train/val), содержащему столбцы: 'audio' и 'text'.\n",
    "        root_dir: корневая папка, содержащая файлы.\n",
    "        max_audio_length: ограничение по длине в сэмплах (обрезаем длинные аудио).\n",
    "        target_sample_rate: ожидаемая частота дискретизации.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "        self.root_dir = root_dir\n",
    "        self.max_audio_length = max_audio_length\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Реализуйте чтение обучаеющего примера.\n",
    "        # Обратите внимание, что частота дискретизации (Sample Rate) должна соотвествовать частоте, на которой обучался аудио энкодер.\n",
    "        # Для ускорения обучения можно заранее отресемплить и векторизовать аудио, чтобы не тратить компьюь при обучении.\n",
    "        # Для дебага наоборот, проще на лету, чтобы не ждать долгую стадию препроцессинга.\n",
    "        # В этой стадии можно ограничить длительность аудио, например, 10 сек\n",
    "        # return waveform, sr, caption\n",
    "        # return vectorized_audio, caption\n",
    "\n",
    "#реализуйте один из вариантов collate_fn для пайплайна с процессингом оффлайн или на лету\n",
    "\n",
    "# def collate_fn(batch, audio_processor: Wav2Vec2Processor):\n",
    "#     \"\"\"\n",
    "#     batch: список из N элементов [(waveform_i, sr_i, caption_i), ...].\n",
    "#     Делает единый вызов audio_processor(..., padding=\"longest\") для всего батча,\n",
    "#     возвращает (audio_inputs, captions).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # audio_inputs[\"input_values\"] => форма [B, T_max]\n",
    "#     # audio_inputs[\"attention_mask\"] => форма [B, T_max]\n",
    "\n",
    "#     return audio_inputs, captions\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     batch: список из N элементов [(waveform_i, sr_i, caption_i), ...].\n",
    "#     Делает единый вызов audio_processor(..., padding=\"longest\") для всего батча,\n",
    "#     возвращает (audio_inputs, captions).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # audio_inputs[\"input_values\"] => форма [B, T_max]\n",
    "#     # audio_inputs[\"attention_mask\"] => форма [B, T_max]\n",
    "\n",
    "#     return audio_inputs, captions\n",
    "\n",
    "train_tsv = \"/content/audiocaps/audiocaps/audiocaps_train_fixed.tsv\"\n",
    "val_tsv   = \"/content/audiocaps/audiocaps/audiocaps_val_new.tsv\"\n",
    "\n",
    "root_dir = \"/content/audiocaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhVAzKNExaze"
   },
   "outputs": [],
   "source": [
    "train_dataset = AudioCapsDataset(train_tsv, root_dir)\n",
    "val_dataset   = AudioCapsDataset(val_tsv, root_dir)\n",
    "# инициализируйте Data loader'ы\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=# your collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=# your collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdT63ULkxXIA",
    "outputId": "a0e46cfb-b204-4a6a-f5eb-f5c13ef0de52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values.shape = torch.Size([4, 160000])\n",
      "attention_mask.shape = torch.Size([4, 160000])\n",
      "captions = ['A man speaking with distant murmuring and clanking', 'Several cat meows', 'A person talking and dribbling a basketball', 'A very aggressive sounding dog']\n"
     ]
    }
   ],
   "source": [
    "# Проверка\n",
    "\n",
    "for batch in train_loader:\n",
    "    audio_inputs, captions = batch\n",
    "    print(\"input_values.shape =\", audio_inputs[\"input_values\"].shape)  # [B, T_max]\n",
    "    print(\"attention_mask.shape =\", audio_inputs[\"attention_mask\"].shape)  # [B, T_max]\n",
    "    print(\"captions =\", captions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IJPPyqspYeH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 3. Трейн QwenAudioDescription (3 балла)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJfYjomHc-wu"
   },
   "source": [
    "1. Напишите класс `QwenAudioDescriptionTrainer`, который будет включать в себя:\n",
    "   - Обучение адаптера (`train_one_epoch`).\n",
    "   - Валидацию (`validate`).\n",
    "   - Генерацию описания аудио (`generate`).\n",
    "2. Реализуйте процесс обучения, который объединяет аудио-эмбеддинги и текстовые токены, а затем передаёт их в QWEN для предсказания текстов.\n",
    "3. Используйте Cross Entropy Loss для оптимизации аудио-адаптера. Остальные параметры модели остаются замороженными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vq4Kj9DNo4Uo"
   },
   "outputs": [],
   "source": [
    "# Создайте ID для специального токена [AUDIO]\n",
    "audio_token_id = qwen_tokenizer(\"[AUDIO]\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "special_token_id = audio_token_id\n",
    "\n",
    "class QwenAudioDescriptionTrainer:\n",
    "    def __init__(self, qwen_model, qwen_tokenizer, audio_encoder, audio_adapter, lr=1e-4):\n",
    "        self.qwen_model = qwen_model\n",
    "        self.qwen_tokenizer = qwen_tokenizer\n",
    "        self.audio_encoder = audio_encoder\n",
    "        self.audio_adapter = audio_adapter\n",
    "\n",
    "        # Создайте оптимизатор Adam (только для audio_adapter)\n",
    "        # self.optimizer = ...\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.audio_adapter.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            audio_inputs, texts = batch  # (audio_inputs, список строк)\n",
    "\n",
    "            # Подготовьте input_values и attn_mask - маску для аттеншена (только аудио, тк текст предсказываем)\n",
    "\n",
    "            # *Этот шаг выполняется, если аудио векторы не вычислялись при формировании батча.\n",
    "            # *Прогните через audio_encoder (заморожен) без градиентов\n",
    "            with torch.no_grad():\n",
    "                # *audio_hidden_states = ...\n",
    "                pass\n",
    "\n",
    "            # Пропустите скрытые состояния через аудио-адаптер\n",
    "            # Токенизируйте текстовые данные и примите QWEN эмбеддер\n",
    "            # Соберите all_embeddings для модели cat(audio, text)\n",
    "            # Таргетом являются лейблы текстовых токенов (описаний аудио)\n",
    "            # Пропустите данные через модель QWEN и вычислите лосс\n",
    "            # Выполните шаг оптимизации\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        return avg_loss\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        self.audio_adapter.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                audio_inputs, texts = batch\n",
    "\n",
    "                # Аналогично train, но без backward\n",
    "                # your code here\n",
    "\n",
    "                pass\n",
    "\n",
    "        return val_loss / len(val_loader)\n",
    "\n",
    "    def generate(self, audio_inputs):\n",
    "        \"\"\"\n",
    "        Генерация описания для одного аудио.\n",
    "        \"\"\"\n",
    "        self.audio_adapter.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # 1) input_values + attn_mask\n",
    "            # 2) audio_encoder -> audio_adapter\n",
    "            # 3) Склейте audio_embeds + небольшой pseudo_input_ids\n",
    "            # 4) Сгенерируйте текст\n",
    "            # generated_ids = ...\n",
    "            # generated_text = ...\n",
    "\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al6T5xqfWjA6"
   },
   "outputs": [],
   "source": [
    "trainer = QwenAudioDescriptionTrainer(...)\n",
    "\n",
    "# your code here\n",
    "# ┌(ಠ_ಠ)┘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cabil9ZVpDiu"
   },
   "source": [
    "> Убедиться, что:\n",
    ">\n",
    "> При обучении лосс падает.\n",
    ">\n",
    "> При валидации всё аналогично, только без backward.\n",
    ">\n",
    ">  При генерации появляется текст (возможно, не самый качественный - это зависит от данных и количества эпох)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZfOOuuephD-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 4. Валидация (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdPZsMIvr6-7"
   },
   "source": [
    "1. Используйте валидационный набор данных, чтобы проверить, насколько хорошо модель генерирует текстовые описания для аудио/картинок.\n",
    "\n",
    "2. Реализуйте процесс генерации текстов для всех аудио/картинок из валидационного набора.\n",
    "\n",
    "3. Используйте метрику **BERTScore** для оценки качества сгенерированных описаний.\n",
    "\n",
    "4. Отобразите примеры сгенерированных текстов и сравните их с истинными описаниями (references)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGwRYSEmM8np"
   },
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "# your code here (＠_＠)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BoFhT78mpSGZ"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CLOP",
   "language": "python",
   "name": "clop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
