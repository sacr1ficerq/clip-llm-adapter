{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzWpMk0BTfno"
   },
   "source": [
    "# ДЗ2. Мультимодальный адаптер к Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOtDtoFXTmY5"
   },
   "source": [
    "**Описание задания**\n",
    "\n",
    "В этом задании вы подключите внешнюю модальность (аудио или изображение) к языковой модели Qwen3-0.6B через небольшой обучаемый адаптер. Веса Qwen и выбранного предобученного энкодера мы замораживаем, обучается только адаптер.\n",
    "\n",
    "- Трек B (изображение → текст)\n",
    "\n",
    "Описываем изображения (image captioning датасет) с помощью Qwen. Энкодер: любая vision-модель.\n",
    "\n",
    "**Задачи:**\n",
    "\n",
    "1.   Заморозить параметры `QWEN` и предобученного энкодера (аудио или vision).\n",
    "\n",
    "2.   Создать и обучить адаптер, который сжимает временную / пространственную размерность признаков и проецирует их в скрытое пространство `Qwen`.\n",
    "\n",
    "3.   Подготовить данные для обучения и валидации модели.\n",
    "\n",
    "4.   Реализовать и сравнить несколько стратегий пулинга в адаптере (например, сжатие временной размерности для аудио или пространственной - для изображений).\n",
    "\n",
    "5.   Использовать `BERTScore` для оценки качества модели на отложенном датасете.\n",
    "\n",
    "**Основные этапы задания**\n",
    "\n",
    "*   Подготовка данных: для \"изображение → текст\" загрузите датасет для image captioning (например, `Flickr8k`), обработайте данные и создайте DataLoader для батчевого обучения. Рекомендуется выполнить полную предобработку данных (прекомпьют), чтобы сократить время обработки на этапе обучения.\n",
    "*   Реализация адаптера: создайте класс, который сжимает последовательность векторов.\n",
    "*   Интеграция с `QWEN`: реализуйте обработку входов и передачу через `QWEN`.\n",
    "*   Обучение модели: настройте процесс обучения с использованием `Cross Entropy Loss` и teacher forcing.\n",
    "*   Оценка: вычислите BERTScore между сгенерированными и реальными текстами на валидационном датасете.\n",
    "\n",
    "> **Внимание!** Последующие ячейки с условиями оформлены (название классов и переменных, инструкциии и комментарии) в ключе работы по треку А (аудио → текст). Если вы выбираете работать с vision задачей, можете ориентироваться содержательно на представленные кодовые сниппеты, но видоизменять их под свою задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ek97gBqjS9U"
   },
   "source": [
    "### Сеттинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JbyKDHKueNBa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloexec/Desktop/HSE/multimodal/HW1/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bert_score import BERTScorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMNWATzFYh67",
    "outputId": "cc5b8b5d-c26f-430d-9784-ab41c92ccf61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ8xbIDFkYy6"
   },
   "source": [
    "**Загрузка данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMH2kpEDbCmU"
   },
   "source": [
    "Если вы выбираете трек с аудио, используйте датасет `AudioCaps`.\n",
    "\n",
    "Я буду использовать Flickr8k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H-IwBsjZ8oBX"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# use flickr8k dataset\n",
    "\n",
    "ds = load_dataset(\"jxie/flickr8k\").with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[ 38,  64,  78,  ...,  52,  60,  41],\n",
       "          [ 59,  91,  54,  ...,  37,  57,  60],\n",
       "          [ 90,  79,  82,  ...,  43,  36,  18],\n",
       "          ...,\n",
       "          [223, 218, 211,  ..., 215, 215, 210],\n",
       "          [241, 234, 233,  ..., 231, 218, 205],\n",
       "          [237, 244, 241,  ..., 226, 226, 223]],\n",
       " \n",
       "         [[ 31,  50,  73,  ...,  48,  56,  37],\n",
       "          [ 37,  59,  34,  ...,  36,  52,  50],\n",
       "          [ 66,  50,  67,  ...,  28,  29,  18],\n",
       "          ...,\n",
       "          [223, 220, 213,  ..., 217, 217, 212],\n",
       "          [243, 236, 238,  ..., 241, 228, 214],\n",
       "          [239, 248, 247,  ..., 230, 229, 226]],\n",
       " \n",
       "         [[ 25,  49,  67,  ...,  37,  47,  28],\n",
       "          [ 49,  60,  25,  ...,  32,  49,  49],\n",
       "          [ 56,  46,  60,  ...,  21,  21,   8],\n",
       "          ...,\n",
       "          [221, 217, 210,  ..., 229, 229, 225],\n",
       "          [240, 233, 234,  ..., 242, 230, 221],\n",
       "          [238, 247, 245,  ..., 239, 238, 233]]], dtype=torch.uint8),\n",
       " 'caption_0': 'A black dog is running after a white dog in the snow .',\n",
       " 'caption_1': 'Black dog chasing brown dog through snow',\n",
       " 'caption_2': 'Two dogs chase each other across the snowy ground .',\n",
       " 'caption_3': 'Two dogs play together in the snow .',\n",
       " 'caption_4': 'Two dogs running through a low lying body of water .'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YGacOadeVci"
   },
   "source": [
    "### Задание 1. Создание и обучение AudioConvAdapter (3 балла)\n",
    "\n",
    "1. Загрузите модель **Qwen3-0.6B** и заморозьте её параметры.  \n",
    "2. Загрузите предобученный аудио-энкодер (параметры также должны быть заморожены). Вы можете выбрать одну из следующих моделей: **HuBERT**, **wav2vec2**, или **Whisper Encoder**. Учтите особенности выбранной модели:  \n",
    "   - Например, `wav2vec2-large-960h-lv60-self` использует `flash_attention`, которая работает только с GPU архитектуры Ampere и с типом данных float16. Это может вызвать сложности, если используется другое оборудование.  \n",
    "3. Реализуйте класс `AudioConvAdapter`, который уменьшает размерность последовательности аудио по времени и переводит её в текстовое пространство модели QWEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4tYDMIDwYkAz"
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "qwen_model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "qwen_tokenizer =  AutoTokenizer.from_pretrained(qwen_model_name, use_fast=True)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(qwen_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello world! This is a simple example of the web application. It's a good idea to have a simple web\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "txt = \"Hello world!\"\n",
    "tok = qwen_tokenizer(txt, return_tensors='pt')\n",
    "tok['input_ids'] = tok['input_ids'].to(device)\n",
    "tok['attention_mask'] = tok['attention_mask'].to(device)\n",
    "out = qwen_model.generate(**tok)\n",
    "qwen_tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze\n",
    "qwen_model.eval()\n",
    "for p in qwen_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4hEbt8GXFhaP"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, CLIPVisionModel\n",
    "\n",
    "# Load\n",
    "vision_model_name = \"openai/clip-vit-base-patch32\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(vision_model_name, use_fast=True)\n",
    "vision_encoder = CLIPVisionModel.from_pretrained(vision_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze\n",
    "vision_encoder.eval()\n",
    "for p in vision_encoder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.adapter import VisionAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gSzOFB4Wbtao"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_hidden = qwen_model.config.hidden_size\n",
    "vision_hidden = vision_encoder.config.hidden_size\n",
    "\n",
    "vision_hidden, qwen_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PoXrO9YFcs2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in adapter: 2,561,280\n"
     ]
    }
   ],
   "source": [
    "adapter_hidden_dim = 1024  # внутренняя размерность adapter’а\n",
    "vision_adapter = VisionAdapter(vision_hidden, qwen_hidden)# your code here\n",
    "\n",
    "print(\"Trainable parameters in adapter: {:,}\".format(sum(p.numel() for p in vision_adapter.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoFhT78mpSGZ"
   },
   "source": [
    "### Задание 2. Подготовка данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import Flickr8kCaptionDataset, collate_fn_vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 21])\n",
      "torch.Size([4, 21])\n",
      "Captions 1-4:\n",
      "\tA man grips the underhang of a rock .\n",
      "\tA man in a wheelchair riding to the park .\n",
      "\tA big brown dog runs with a stick in his mouth , and a big black down runs behind him .\n",
      "\tA brown dogs licks its black nose .\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Flickr8kCaptionDataset(ds[\"train\"])\n",
    "val_dataset   = Flickr8kCaptionDataset(ds[\"test\"] if \"test\" in ds else ds[\"validation\"])\n",
    "\n",
    "# DataLoaders (note: collate_fn needs processor+tokenizer, so use lambda/partial)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    # num_workers=4,\n",
    "    # pin_memory=True,\n",
    "    collate_fn=lambda b: collate_fn_vision(b, image_processor, qwen_tokenizer, device),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    # num_workers=4,\n",
    "    # pin_memory=True,\n",
    "    collate_fn=lambda b: collate_fn_vision(b, image_processor, qwen_tokenizer, device),\n",
    ")\n",
    "\n",
    "\n",
    "# Проверка\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[\"pixel_values\"].shape)      # (B, 3, H, W)\n",
    "print(batch[\"input_ids\"].shape)         # (B, L)\n",
    "print(batch[\"attention_mask\"].shape)    # (B, L)\n",
    "print('Captions 1-4:', end='\\n\\t')\n",
    "print(*batch[\"captions\"][:4], sep='\\n\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual_embeds: torch.Size([4, 16, 1024])\n"
     ]
    }
   ],
   "source": [
    "vision_adapter = vision_adapter.to(device).train()\n",
    "vision_encoder.eval()\n",
    "qwen_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    vis_out = vision_encoder(pixel_values=batch[\"pixel_values\"])\n",
    "    vis_tokens = vis_out.last_hidden_state  # (B, S, Dv)\n",
    "\n",
    "visual_embeds = vision_adapter(vis_tokens)  # (B, T, Dq)\n",
    "print(\"visual_embeds:\", visual_embeds.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IJPPyqspYeH"
   },
   "source": [
    "## Задание 3. Трейн QwenAudioDescription (3 балла)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJfYjomHc-wu"
   },
   "source": [
    "1. Напишите класс `QwenAudioDescriptionTrainer`, который будет включать в себя:\n",
    "   - Обучение адаптера (`train_one_epoch`).\n",
    "   - Валидацию (`validate`).\n",
    "   - Генерацию описания аудио (`generate`).\n",
    "2. Реализуйте процесс обучения, который объединяет аудио-эмбеддинги и текстовые токены, а затем передаёт их в QWEN для предсказания текстов.\n",
    "3. Используйте Cross Entropy Loss для оптимизации аудио-адаптера. Остальные параметры модели остаются замороженными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vq4Kj9DNo4Uo"
   },
   "outputs": [],
   "source": [
    "# # Создайте ID для специального токена [AUDIO]\n",
    "# audio_token_id = qwen_tokenizer(\"[AUDIO]\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "# special_token_id = audio_token_id\n",
    "\n",
    "# class QwenAudioDescriptionTrainer:\n",
    "#     def __init__(self, qwen_model, qwen_tokenizer, audio_encoder, audio_adapter, lr=1e-4):\n",
    "#         self.qwen_model = qwen_model\n",
    "#         self.qwen_tokenizer = qwen_tokenizer\n",
    "#         self.audio_encoder = audio_encoder\n",
    "#         self.audio_adapter = audio_adapter\n",
    "\n",
    "#         # Создайте оптимизатор Adam (только для audio_adapter)\n",
    "#         # self.optimizer = ...\n",
    "\n",
    "#     def train_one_epoch(self, train_loader):\n",
    "#         self.audio_adapter.train()\n",
    "#         total_loss = 0.0\n",
    "\n",
    "#         for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "#             audio_inputs, texts = batch  # (audio_inputs, список строк)\n",
    "\n",
    "#             # Подготовьте input_values и attn_mask - маску для аттеншена (только аудио, тк текст предсказываем)\n",
    "\n",
    "#             # *Этот шаг выполняется, если аудио векторы не вычислялись при формировании батча.\n",
    "#             # *Прогните через audio_encoder (заморожен) без градиентов\n",
    "#             with torch.no_grad():\n",
    "#                 # *audio_hidden_states = ...\n",
    "#                 pass\n",
    "\n",
    "#             # Пропустите скрытые состояния через аудио-адаптер\n",
    "#             # Токенизируйте текстовые данные и примите QWEN эмбеддер\n",
    "#             # Соберите all_embeddings для модели cat(audio, text)\n",
    "#             # Таргетом являются лейблы текстовых токенов (описаний аудио)\n",
    "#             # Пропустите данные через модель QWEN и вычислите лосс\n",
    "#             # Выполните шаг оптимизации\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "#         return avg_loss\n",
    "\n",
    "#     def validate(self, val_loader):\n",
    "#         self.audio_adapter.eval()\n",
    "#         val_loss = 0.0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "#                 audio_inputs, texts = batch\n",
    "\n",
    "#                 # Аналогично train, но без backward\n",
    "#                 # your code here\n",
    "\n",
    "#                 pass\n",
    "\n",
    "#         return val_loss / len(val_loader)\n",
    "\n",
    "#     def generate(self, audio_inputs):\n",
    "#         \"\"\"\n",
    "#         Генерация описания для одного аудио.\n",
    "#         \"\"\"\n",
    "#         self.audio_adapter.eval()\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             # 1) input_values + attn_mask\n",
    "#             # 2) audio_encoder -> audio_adapter\n",
    "#             # 3) Склейте audio_embeds + небольшой pseudo_input_ids\n",
    "#             # 4) Сгенерируйте текст\n",
    "#             # generated_ids = ...\n",
    "#             # generated_text = ...\n",
    "\n",
    "#         return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import QwenVisionCaptionTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QwenVisionCaptionTrainer(\n",
    "    qwen_model=qwen_model,\n",
    "    qwen_tokenizer=qwen_tokenizer,\n",
    "    vision_encoder=vision_encoder,\n",
    "    vision_adapter=vision_adapter.to(device),\n",
    "    device=device,\n",
    "    lr=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:59<00:00,  8.33it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 train_loss=3.3811 val_loss=3.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [03:01<00:00,  8.27it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 train_loss=3.0188 val_loss=2.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for ep in range(1, epochs + 1):\n",
    "    tr_loss = trainer.train_one_epoch(train_loader)\n",
    "    va_loss = trainer.validate(val_loader)\n",
    "    print(f\"epoch={ep} train_loss={tr_loss:.4f} val_loss={va_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cabil9ZVpDiu"
   },
   "source": [
    "> Убедиться, что:\n",
    ">\n",
    "> При обучении лосс падает.\n",
    ">\n",
    "> При валидации всё аналогично, только без backward.\n",
    ">\n",
    ">  При генерации появляется текст (возможно, не самый качественный - это зависит от данных и количества эпох)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:57<00:00,  8.43it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 train_loss=2.8919 val_loss=2.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:58<00:00,  8.40it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 train_loss=2.7888 val_loss=2.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:58<00:00,  8.38it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3 train_loss=2.6978 val_loss=2.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:58<00:00,  8.40it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4 train_loss=2.6067 val_loss=2.7766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [03:00<00:00,  8.32it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5 train_loss=2.5293 val_loss=3.5198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [03:04<00:00,  8.15it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6 train_loss=2.6476 val_loss=2.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [03:01<00:00,  8.26it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7 train_loss=2.5109 val_loss=2.7161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [03:02<00:00,  8.20it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:18<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8 train_loss=2.4053 val_loss=2.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:59<00:00,  8.36it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9 train_loss=2.3043 val_loss=2.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:58<00:00,  8.41it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10 train_loss=2.1957 val_loss=2.7852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for ep in range(1, epochs + 1):\n",
    "    tr_loss = trainer.train_one_epoch(train_loader)\n",
    "    va_loss = trainer.validate(val_loader)\n",
    "    print(f\"epoch={ep} train_loss={tr_loss:.4f} val_loss={va_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZfOOuuephD-"
   },
   "source": [
    "## Задание 4. Валидация (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdPZsMIvr6-7"
   },
   "source": [
    "1. Используйте валидационный набор данных, чтобы проверить, насколько хорошо модель генерирует текстовые описания для аудио/картинок.\n",
    "\n",
    "2. Реализуйте процесс генерации текстов для всех аудио/картинок из валидационного набора.\n",
    "\n",
    "3. Используйте метрику **BERTScore** для оценки качества сгенерированных описаний.\n",
    "\n",
    "4. Отобразите примеры сгенерированных текстов и сравните их с истинными описаниями (references)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [05:14<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "preds, refs = [], []\n",
    "for batch in tqdm(val_loader, desc=\"Generating\"):\n",
    "    preds.extend(trainer.generate(batch[\"pixel_values\"], max_new_tokens=40, num_beams=3))\n",
    "    refs.extend(batch[\"captions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore P=-0.3885 R=0.3047 F1=-0.0696\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(model_type=\"roberta-large\", lang=\"en\", rescale_with_baseline=True)  # rescale supported by bert-score\n",
    "P, R, F1 = scorer.score(preds, refs)\n",
    "\n",
    "print(f\"BERTScore P={P.mean().item():.4f} R={R.mean().item():.4f} F1={F1.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REF: The dogs are in the snow in front of a fence .\n",
      "HYP: A brown dog is running in the snow . The other dog is a black dog . The dogs are both dogs . The dogs are both dogs . The dogs are both dogs . The dogs are both dogs\n",
      "\n",
      "REF: a brown and white dog swimming towards some in the pool\n",
      "HYP: a black and white dog jumping into a pool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "REF: A man and a woman in festive costumes dancing .\n",
      "HYP: A man in a white shirt and a black shirt is holding a sign that says “I’m a man.” ” . . . . . . . . . . . . . . . . .\n",
      "\n",
      "REF: A couple of people sit outdoors at a table with an umbrella and talk .\n",
      "HYP: A man and a woman sit on a bench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "REF: A man is wearing a Sooners red football shirt and helmet .\n",
      "HYP: A black and white football player in a red jersey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"\\nREF:\", refs[i])\n",
    "    print(\"HYP:\", preds[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BoFhT78mpSGZ"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CLOP",
   "language": "python",
   "name": "clop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
